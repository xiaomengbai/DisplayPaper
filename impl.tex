\section{Implementation}
\label{sec:implementation}



We integrated our LCD-GDP scheme into the WebRTC open source
project~\cite{webrtcproject}, which is the WebRTC component of
the Chrome browser implemented by Google. Then this component can be
linked to the WebRTC app, an Android app, and then we use this app to
do the evluation. 

The scanning module is implemented in C++ and is hooked after where
the captured frames are generated.  In practice, the YUV image is
produced by the camera on Android mobile devices, and the scanning
module directly extracts the Y component of each pixel, which
represents the luminance. Then we select the maximum value of Y among
all pixels and encode it into the Y data panel.  Because frame
information may be lost due to packet lost during network transmission
or due to video compression, we encode the maximum luminance value in
multiple positions in the Y data panel.  After that, the updated
frames are sent to the encoder. There is one scanning module in each
client that participates in the video call.

The adjustment and the rendering modules are both implemented at the
JAVA layer.  One adjustment module and one rendering module are
required for each video stream, including the stream produced by the
host itself, for processing and rendering frames of this stream.  For
example, if a video call involves two devices, there will two
adjustment modules and two rendering modules on each device to process
two streams.  For each stream, these two modules run on independent
threads and are connected by a YUV frame queue.

The adjustment module receives the YUV frames from the decoder and 
reads the maximum frame luminance information that is encoded in each frame.
Since this information is encoded in multiple places and some may be
corrupted or lost,
we conservatively select the greatest value among the luminance candidates.
%we select the maximum luminance value that is agreed on by major votes.
We use this maximum luminance information to determine the future backlight intensity 
based on our greedy algorithm when rendering this frame.
%However, the luminance information may be lost due to packet lost during network transmission 
%or due to video compression, we 
%maximum luminance information by election {\bf not sure what election
%  mean here?} and determines the future
%backlight intensity according to our greedy algorithm when rendering this frame. 
After that, the frame is enqueued to be processed by the rendering module, 
and a three-tuple ({\tt stream-id}, {\tt frame index}, 
{\tt adjusted backlight intensity}) is stored into a global hash
table. 

Given that a device has to render at least two streams 
(one from itself and the other from the other end of the call) in a video call,
LCD-GDP collects the backlight intensity candidates by using the index of the next
frame combined with its stream-id to look up the hash table.
The candidate with the greatest value is selected, and 
backlight scaling and pixel compensation are performed based on the selected value.

Video call frames are rendered by invoking the rendering modules
sequentially. It fetches the YUV frame from the queue, and uses the
OpenGL ES Shaders to do resizing, luminance compensation and YUV to RGB
conversation. Eventually the framebuffer generated by the shaders is
flushed to the screen. 

%% songqing

%% We integrated our LCD-GDP scheme into the WebRTC open
%% source project~\cite{webrtcproject}, which is the WebRTC implementation of the Chrome
%% browser. 


%% The scanning module is implemented in C++ and is hooked after
%% where the captured frames are generated.  In practice, the YUV image
%% is produced by the camera on Android mobile devices, and the scanning
%% module directly extracts the Y component of each pixel, which represents
%% the luminance. Then the resulting Y value is written into the Y data
%% panel in multiple positions. After that, the updated frames are sent
%% to the encoder. There is one scanning module  in each client.

%% The adjustment and the rendering modules are both
%% implemented at the JAVA layer. For each
%% video stream, including the host itself, there is one adjustment
%% module and one rendering module, respectively, to process the
%% frames of this stream. For example, there are two adjustment modules and two
%% rendering modules on each client if two devices are used in
%% the video call. For each stream, these two modules run on
%% independent threads, and are connected by a queue. The 
%%   adjustment module receives the YUV frames from the decoder, reads the
%% maximum luminance information by election {\bf not sure what election
%%   mean here?} and determines the future
%% backlight intensity according to our greedy algorithm when rendering this frame. After that,
%% the processed frame is  enqueued and a three-tuple ({\tt stream-id},
%% {\tt frame index}, 
%% {\tt adjusted backlight intensity}) is stored into a global hash
%% table. The rendering operation is to invoke the rendering modules
%% sequentially. Before doing that, LCD-GDP collects the corresponding
%% backlight intensity candidates by using the index of next playing
%% frame combined with its stream-id. The greatest candidate is used
%% to do the backlight scaling and pixel compensation. When the rendering
%% module is invoked, it fetches the frame from the queue, and uses the
%% OpenGL ES Shaders to do resizing, luminance compensation and YUV-RGB
%% conversation. Eventually the framebuffer generated by the shaders is
%% flushed to the screen. 



%% We integrated our LCD-GDP into the WebRTC~\cite{webrtcproject} open
%% source project, which is the WebRTC implementation of the Chrome
%% browser. 


%% The scanning module is implemented in C++ and is hooked after
%% where the captured frames are generated.  In practice, the YUV image
%% is produced by the camera on Android mobile devices, and the scanning
%% module directly extracts the Y component of each pixel, which represents
%% the luminance. Then the resulting Y value is written into the Y data
%% panel in multiple positions. After that, the updated frames are sent
%% to the encoder. Only one scanning module is located in each client.

%% The adjustment and the rendering module are both
%% implemented at the JAVA layer. For each
%% video stream, including the host itself, there is one adjustment
%% module and one rendering module respectively to process the
%% frames of this stream. For example, there are two adjustment modules and two
%% rendering modules module on each client if two devices are involved in
%% this conversation. For each stream, these two modules run on
%% independent threads, and are connected by a queue. The 
%%   adjustment module receives the YUV frames from the decoder, reads the
%% maximum luminance information by election and determines the future
%% backlight intensity greedily when rendering this frame. After that,
%% the processed frame is  enqueued and one three-tuple of stream-id, frame index and
%% adjusted backlight intensity is stored into a global hash
%% table. The rendering operation is to invoke the rendering modules
%% sequentially. Before doing that, LCD-GDP collects the corresponding
%% backlight intensity candidates by using the index of next playing
%% frame combined with its stream-id. The greatest candidate is used
%% to do the backlight scaling and pixel compensation. When the rendering
%% module is invoked, it fetch the frame from the queue, and use the
%% Opengl ES Shaders to do resizing, luminance compensation and YUV-RGB
%% conversation. Eventually the framebuffer generated by the shaders is
%% flushed to the screen. 

